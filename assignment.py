# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CvCApyJhSh9SA0N9qpbbcF_hTrgdsmdU

# Importing Libraries
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, f1_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import matplotlib.pyplot as plt
import seaborn as sns

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""# Importing training and test data

"""

# Load datasets
train_df = pd.read_csv("/content/train (1).csv", header=None, names=['abstract', 'target'])
test_df = pd.read_csv("/content/validation.csv", header=None, names=['abstract', 'target'])

train_df.head()

test_df.head()

"""# Preprocessing"""

# checking null values
train_df.isnull().sum()

test_df.isnull().sum()

train_df['target'].value_counts()

# Preprocessing function
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\W', ' ', text)
    text = re.sub(r'\d', ' ', text)
    text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text)
    text = re.sub(r'\^[a-zA-Z]\s+', ' ', text)
    text = re.sub(r'\s+', ' ', text, flags=re.I)

    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])

    return text

train_df['processed_abstract'] = train_df['abstract'].apply(preprocess_text)
test_df['processed_abstract'] = test_df['abstract'].apply(preprocess_text)

# Encode categories
le = LabelEncoder()
train_df['encoded_category'] = le.fit_transform(train_df['target'])
test_df['encoded_category'] = le.transform(test_df['target'])

# Handle class imbalance with SMOTE
smote = SMOTE(random_state=42)
X = train_df['processed_abstract']
y = train_df['encoded_category']

tfidf = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf.fit_transform(X)

X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)

X_test_tfidf = tfidf.transform(test_df['processed_abstract'])
y_test = test_df['encoded_category']

"""# Predicting results using logistic regression"""

from sklearn.linear_model import LogisticRegression

# Train logistic regression model
logreg = LogisticRegression(max_iter=1000, random_state=42)
logreg.fit(X_resampled, y_resampled)

# Predict and evaluate
y_pred_logreg = logreg.predict(X_test_tfidf)
print("Logistic Regression:")
print(classification_report(y_test, y_pred_logreg, target_names=le.classes_))

"""
# Predicting results using SVM"""

from sklearn.svm import SVC

# Train SVM model
svm = SVC(kernel='linear', random_state=42)
svm.fit(X_resampled, y_resampled)

# Predict and evaluate
y_pred_svm = svm.predict(X_test_tfidf)
print("SVM:")
print(classification_report(y_test, y_pred_svm, target_names=le.classes_))

"""# Predicting results using XGBoost"""

from xgboost import XGBClassifier

# Train XGBoost model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb.fit(X_resampled, y_resampled)

# Predict and evaluate
y_pred_xgb = xgb.predict(X_test_tfidf)
print("XGBoost:")
print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))

"""
# Predicting results using LSTM"""

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D
from keras.utils import to_categorical

# Tokenize and pad sequences
tokenizer = Tokenizer(num_words=5000, lower=True)
tokenizer.fit_on_texts(train_df['processed_abstract'])
X_train_seq = tokenizer.texts_to_sequences(train_df['processed_abstract'])
X_test_seq = tokenizer.texts_to_sequences(test_df['processed_abstract'])

max_sequence_length = 500
X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length)

# Convert labels to categorical
y_train_cat = to_categorical(train_df['encoded_category'])
y_test_cat = to_categorical(test_df['encoded_category'])

# Build LSTM model
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_sequence_length))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(7, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_pad, y_train_cat, epochs=5, batch_size=64, validation_split=0.2, verbose=2)

# Predict and evaluate
y_pred_lstm = model.predict(X_test_pad)
y_pred_lstm_classes = np.argmax(y_pred_lstm, axis=1)
print("LSTM:")
print(classification_report(y_test, y_pred_lstm_classes, target_names=le.classes_))

# Cross-validation for Logistic Regression
cv_scores_logreg = cross_val_score(logreg, X_resampled, y_resampled, cv=5, scoring='f1_weighted')
print("Logistic Regression Cross-Validation F1 Scores:", cv_scores_logreg)
print("Mean Logistic Regression Cross-Validation F1 Score:", np.mean(cv_scores_logreg))

# Learning Curves for Logistic Regression
train_sizes, train_scores, test_scores = learning_curve(logreg, X_resampled, y_resampled, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5))
plt.figure()
plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', color='r', label='Training score')
plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', color='g', label='Cross-validation score')
plt.title('Learning Curves (Logistic Regression)')
plt.xlabel('Training Size')
plt.ylabel('Score')
plt.legend(loc='best')
plt.show()

"""# Confusion matrix for Logistic Regression, XGBoost and SVM"""

from sklearn.metrics import confusion_matrix

# Confusion Matrix for the best performing model (e.g., XGBoost)
conf_mat = confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix (XGBoost)')
plt.show()

import pandas as pd

# Confusion Matrix for the best performing model (e.g., XGBoost)
conf_mat = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix (SVM)')
plt.show()

# Confusion Matrix for the best performing model (e.g., XGBoost)
conf_mat = confusion_matrix(y_test, y_pred_logreg)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix (Logistic)')
plt.show()

